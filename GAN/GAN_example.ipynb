{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt9Rz/XNhqeylKsJ4cFBNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richardhzj/ML/blob/main/GAN/GAN_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z2bdjm7H85j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from keras.datasets import mnist\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        # enter a low dimension vector first 128x100, then convert to image size 56x56\n",
        "        self.fc1 = nn.Linear(input_dim, 56 * 56)\n",
        "        self.br = nn.Sequential(\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(True) # inplace设为True，让操作在原地进行\n",
        "        )\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 50, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(50),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(50, 25, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(25),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(25, 1, 2, stride=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = x.view(-1, 1, 56, 56)\n",
        "        x = self.br(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        output = self.conv3(x)\n",
        "        return output # [128,1,28,28]"
      ],
      "metadata": {
        "id": "GoajgDWnImRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 5, stride=1, padding=2),\n",
        "            nn.LeakyReLU(0.2,True)\n",
        "        )\n",
        "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, stride=1, padding=2),\n",
        "            nn.LeakyReLU(0.2,True)\n",
        "        )\n",
        "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 1024),\n",
        "            nn.LeakyReLU(0.2,True)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(1024, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pl1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pl2(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        output = self.fc2(x)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "qvfVHViiIqpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def G_train(input_dim):\n",
        "    G_optimizer.zero_grad()\n",
        "\n",
        "    noise = torch.randn(batch_size, input_dim).to(device)\n",
        "    # we want to generate real image, so labels are 1\n",
        "    real_label = torch.ones(batch_size,1).to(device)\n",
        "    fake_img = G(noise)\n",
        "    D_output = D(fake_img)\n",
        "    G_loss = criterion(D_output, real_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_loss.data.item()"
      ],
      "metadata": {
        "id": "OqnVYj1KIuJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def D_train(real_img, input_dim):\n",
        "    # torch.Size([100, 1, 28, 28])\n",
        "    D_optimizer.zero_grad()\n",
        "\n",
        "    real_label = torch.ones(real_img.shape[0],1).to(device)\n",
        "    D_output = D(real_img)\n",
        "    # print(\"D output\",D_output.shape)\n",
        "    # print(\"real label\", real_label.shape)\n",
        "    D_real_loss = criterion(D_output, real_label)\n",
        "\n",
        "    noise = torch.randn(batch_size, input_dim, requires_grad=False).to(device)\n",
        "    fake_label = torch.zeros(batch_size,1).to(device)\n",
        "    fake_img = G(noise)\n",
        "    D_output = D(fake_img.detach()) # 要不要用fake_img.detach()。 参与计算，变量中涉及的梯度就会自动计算？\n",
        "    D_fake_loss = criterion(D_output, fake_label)\n",
        "\n",
        "    D_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    return D_loss.data.item()"
      ],
      "metadata": {
        "id": "FxueJiCkIxxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_img(img, img_name):\n",
        "\n",
        "    # when generating GAN, we normalize pixel to range[-1,1]. this will help rescale to [0,1]\n",
        "    img = 0.5 * (img + 1)\n",
        "    img = img.clamp(0, 1)\n",
        "    save_image(img, \"./imgs/\" + img_name)\n",
        "    # print(\"image has saved.\")\n"
      ],
      "metadata": {
        "id": "ddLDf-QNI0y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "epoch_num = 30\n",
        "lr = 0.0002\n",
        "input_dim = 100"
      ],
      "metadata": {
        "id": "yVlvf8M-I_pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch, (x, _) in enumerate(train_loader):\n",
        "#   print(x.shape)\n",
        "#   print(batch)\n",
        "#   break\n",
        "# print(len(train_loader))"
      ],
      "metadata": {
        "id": "xHXJnPwGJdg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fake_img = torch.randn(128, input_dim)\n",
        "# fake_img = G(fake_img.to(device))\n",
        "# print(fake_img.shape)\n",
        "\n",
        "# a = fake_img = 0.5 * (fake_img + 1)\n",
        "# print(a.shape)"
      ],
      "metadata": {
        "id": "ucTyrS1yfHdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if not os.path.exists(\"./checkpoint\"):\n",
        "        os.makedirs(\"./checkpoint\")\n",
        "\n",
        "    if not os.path.exists(\"./imgs\"):\n",
        "        os.makedirs(\"./imgs\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 加载数据\n",
        "    train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=torchvision.transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 构建生成器和判别器网络\n",
        "    if os.path.exists('./checkpoint/Generator.pkl') and os.path.exists('./checkpoint/Discriminator.pkl'):\n",
        "        G=torch.load(\"./checkpoint/Generator.pkl\").to(device)\n",
        "        D=torch.load(\"./checkpoint/Discriminator.pkl\").to(device)\n",
        "    else:\n",
        "        G = Generator(input_dim).to(device)\n",
        "        D = Discriminator().to(device)\n",
        "\n",
        "    # 指明损失函数和优化器\n",
        "    criterion = nn.BCELoss()\n",
        "    G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
        "    D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
        "\n",
        "    epoch_D_loss = []\n",
        "    epoch_G_loss = []\n",
        "    \n",
        "    print(\"Training...........\")\n",
        "    for epoch in range(1, epoch_num + 1):\n",
        "        d_epoch_loss = 0\n",
        "        g_epoch_loss = 0\n",
        "        count = len(train_loader)\n",
        "        print(\"epoch: \", epoch)\n",
        "        for batch, (x, _) in enumerate(train_loader):\n",
        "            # 对判别器和生成器分别进行训练，注意顺序不能反\n",
        "            D_loss=D_train(x.to(device), input_dim)\n",
        "            G_loss=G_train(input_dim)\n",
        "\n",
        "            #if batch % 20 == 0:\n",
        "\n",
        "            print(\"[ %d / %d ]  g_loss: %.6f  d_loss: %.6f\" % (batch, 600, float(G_loss), float(D_loss)))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                d_epoch_loss+=D_loss\n",
        "                g_epoch_loss+=G_loss\n",
        "\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                fake_img = torch.randn(128, input_dim)\n",
        "                fake_img = G(fake_img.to(device))\n",
        "                \n",
        "                save_img(fake_img, \"img_\" + str(epoch) + \"_\" + str(batch) + \".png\")\n",
        "                # 保存模型\n",
        "                torch.save(G, \"./checkpoint/Generator.pkl\")\n",
        "                torch.save(D, \"./checkpoint/Discriminator.pkl\")\n",
        "    with torch.no_grad():\n",
        "        d_epoch_loss/=count\n",
        "        g_epoch_loss/=count\n",
        "        epoch_D_loss.append(d_epoch_loss)\n",
        "        epoch_G_loss.append(g_epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5p54-ptI3xE",
        "outputId": "06990c72-b727-4019-93dc-1daf423eeff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...........\n",
            "epoch:  1\n",
            "[ 0 / 600 ]  g_loss: 5.261096  d_loss: 0.432289\n",
            "[ 1 / 600 ]  g_loss: 1.316787  d_loss: 0.758210\n",
            "[ 2 / 600 ]  g_loss: 1.608086  d_loss: 0.629391\n",
            "[ 3 / 600 ]  g_loss: 3.068643  d_loss: 0.623034\n",
            "[ 4 / 600 ]  g_loss: 3.878047  d_loss: 0.430954\n",
            "[ 5 / 600 ]  g_loss: 4.065329  d_loss: 0.455633\n",
            "[ 6 / 600 ]  g_loss: 2.880516  d_loss: 0.461076\n",
            "[ 7 / 600 ]  g_loss: 1.947001  d_loss: 0.338868\n",
            "[ 8 / 600 ]  g_loss: 1.991178  d_loss: 0.480254\n",
            "[ 9 / 600 ]  g_loss: 2.413177  d_loss: 0.478868\n",
            "[ 10 / 600 ]  g_loss: 2.997023  d_loss: 0.512879\n",
            "[ 11 / 600 ]  g_loss: 3.250618  d_loss: 0.525221\n",
            "[ 12 / 600 ]  g_loss: 2.680915  d_loss: 0.428168\n",
            "[ 13 / 600 ]  g_loss: 2.276666  d_loss: 0.391689\n",
            "[ 14 / 600 ]  g_loss: 2.456187  d_loss: 0.480449\n",
            "[ 15 / 600 ]  g_loss: 2.726231  d_loss: 0.316564\n",
            "[ 16 / 600 ]  g_loss: 2.594761  d_loss: 0.419789\n",
            "[ 17 / 600 ]  g_loss: 2.908499  d_loss: 0.334996\n",
            "[ 18 / 600 ]  g_loss: 2.908427  d_loss: 0.388803\n",
            "[ 19 / 600 ]  g_loss: 2.418208  d_loss: 0.410772\n",
            "[ 20 / 600 ]  g_loss: 2.301667  d_loss: 0.472739\n",
            "[ 21 / 600 ]  g_loss: 2.167763  d_loss: 0.447479\n",
            "[ 22 / 600 ]  g_loss: 2.346219  d_loss: 0.416525\n",
            "[ 23 / 600 ]  g_loss: 2.737616  d_loss: 0.389281\n",
            "[ 24 / 600 ]  g_loss: 2.894177  d_loss: 0.324188\n",
            "[ 25 / 600 ]  g_loss: 3.019861  d_loss: 0.452979\n",
            "[ 26 / 600 ]  g_loss: 2.496490  d_loss: 0.418368\n",
            "[ 27 / 600 ]  g_loss: 2.053565  d_loss: 0.479872\n",
            "[ 28 / 600 ]  g_loss: 2.349513  d_loss: 0.553003\n",
            "[ 29 / 600 ]  g_loss: 2.671091  d_loss: 0.379190\n",
            "[ 30 / 600 ]  g_loss: 2.938639  d_loss: 0.475272\n",
            "[ 31 / 600 ]  g_loss: 2.893977  d_loss: 0.410227\n",
            "[ 32 / 600 ]  g_loss: 2.577409  d_loss: 0.344427\n",
            "[ 33 / 600 ]  g_loss: 2.369859  d_loss: 0.358341\n",
            "[ 34 / 600 ]  g_loss: 2.186560  d_loss: 0.313598\n",
            "[ 35 / 600 ]  g_loss: 2.202661  d_loss: 0.504343\n",
            "[ 36 / 600 ]  g_loss: 2.199910  d_loss: 0.343694\n",
            "[ 37 / 600 ]  g_loss: 2.824540  d_loss: 0.408957\n",
            "[ 38 / 600 ]  g_loss: 3.003592  d_loss: 0.411584\n",
            "[ 39 / 600 ]  g_loss: 2.696088  d_loss: 0.426189\n",
            "[ 40 / 600 ]  g_loss: 3.114547  d_loss: 0.291880\n",
            "[ 41 / 600 ]  g_loss: 2.418426  d_loss: 0.354503\n",
            "[ 42 / 600 ]  g_loss: 2.135252  d_loss: 0.368680\n",
            "[ 43 / 600 ]  g_loss: 2.362967  d_loss: 0.372877\n",
            "[ 44 / 600 ]  g_loss: 2.777555  d_loss: 0.338974\n",
            "[ 45 / 600 ]  g_loss: 2.807025  d_loss: 0.383034\n",
            "[ 46 / 600 ]  g_loss: 2.955566  d_loss: 0.322247\n",
            "[ 47 / 600 ]  g_loss: 2.335065  d_loss: 0.444690\n",
            "[ 48 / 600 ]  g_loss: 1.797746  d_loss: 0.375225\n",
            "[ 49 / 600 ]  g_loss: 2.079157  d_loss: 0.537942\n",
            "[ 50 / 600 ]  g_loss: 3.149258  d_loss: 0.479281\n",
            "[ 51 / 600 ]  g_loss: 3.549527  d_loss: 0.373765\n",
            "[ 52 / 600 ]  g_loss: 3.277956  d_loss: 0.397060\n",
            "[ 53 / 600 ]  g_loss: 2.139261  d_loss: 0.459273\n",
            "[ 54 / 600 ]  g_loss: 1.853586  d_loss: 0.388212\n",
            "[ 55 / 600 ]  g_loss: 2.195587  d_loss: 0.441829\n",
            "[ 56 / 600 ]  g_loss: 3.163835  d_loss: 0.405610\n",
            "[ 57 / 600 ]  g_loss: 3.364889  d_loss: 0.406346\n",
            "[ 58 / 600 ]  g_loss: 3.333442  d_loss: 0.423935\n",
            "[ 59 / 600 ]  g_loss: 2.457594  d_loss: 0.405608\n",
            "[ 60 / 600 ]  g_loss: 1.981889  d_loss: 0.334478\n",
            "[ 61 / 600 ]  g_loss: 2.500700  d_loss: 0.389544\n",
            "[ 62 / 600 ]  g_loss: 3.293174  d_loss: 0.361022\n",
            "[ 63 / 600 ]  g_loss: 3.550450  d_loss: 0.409646\n",
            "[ 64 / 600 ]  g_loss: 2.971685  d_loss: 0.406041\n",
            "[ 65 / 600 ]  g_loss: 2.289440  d_loss: 0.440907\n",
            "[ 66 / 600 ]  g_loss: 1.831647  d_loss: 0.421672\n",
            "[ 67 / 600 ]  g_loss: 2.290233  d_loss: 0.439438\n",
            "[ 68 / 600 ]  g_loss: 2.686667  d_loss: 0.541021\n",
            "[ 69 / 600 ]  g_loss: 2.814641  d_loss: 0.461863\n",
            "[ 70 / 600 ]  g_loss: 2.985681  d_loss: 0.252099\n",
            "[ 71 / 600 ]  g_loss: 2.274427  d_loss: 0.494456\n",
            "[ 72 / 600 ]  g_loss: 1.957908  d_loss: 0.451229\n",
            "[ 73 / 600 ]  g_loss: 2.666559  d_loss: 0.449215\n",
            "[ 74 / 600 ]  g_loss: 3.292407  d_loss: 0.372441\n",
            "[ 75 / 600 ]  g_loss: 3.251098  d_loss: 0.452733\n",
            "[ 76 / 600 ]  g_loss: 3.024450  d_loss: 0.277337\n",
            "[ 77 / 600 ]  g_loss: 3.043474  d_loss: 0.354779\n",
            "[ 78 / 600 ]  g_loss: 2.252325  d_loss: 0.448072\n",
            "[ 79 / 600 ]  g_loss: 1.901840  d_loss: 0.377964\n",
            "[ 80 / 600 ]  g_loss: 2.201020  d_loss: 0.412693\n",
            "[ 81 / 600 ]  g_loss: 2.940811  d_loss: 0.441580\n",
            "[ 82 / 600 ]  g_loss: 3.621076  d_loss: 0.306814\n",
            "[ 83 / 600 ]  g_loss: 3.284195  d_loss: 0.506054\n",
            "[ 84 / 600 ]  g_loss: 2.499355  d_loss: 0.323461\n",
            "[ 85 / 600 ]  g_loss: 1.802830  d_loss: 0.411416\n",
            "[ 86 / 600 ]  g_loss: 1.775270  d_loss: 0.375237\n",
            "[ 87 / 600 ]  g_loss: 3.076917  d_loss: 0.433606\n",
            "[ 88 / 600 ]  g_loss: 3.893049  d_loss: 0.406353\n",
            "[ 89 / 600 ]  g_loss: 4.012943  d_loss: 0.502565\n",
            "[ 90 / 600 ]  g_loss: 2.576551  d_loss: 0.672385\n",
            "[ 91 / 600 ]  g_loss: 1.546265  d_loss: 0.348875\n",
            "[ 92 / 600 ]  g_loss: 1.680109  d_loss: 0.488348\n",
            "[ 93 / 600 ]  g_loss: 2.854816  d_loss: 0.504466\n",
            "[ 94 / 600 ]  g_loss: 4.441386  d_loss: 0.378332\n",
            "[ 95 / 600 ]  g_loss: 4.571376  d_loss: 0.555360\n",
            "[ 96 / 600 ]  g_loss: 3.319141  d_loss: 0.670420\n",
            "[ 97 / 600 ]  g_loss: 1.930041  d_loss: 0.421825\n",
            "[ 98 / 600 ]  g_loss: 1.623831  d_loss: 0.452873\n",
            "[ 99 / 600 ]  g_loss: 2.281318  d_loss: 0.553287\n",
            "[ 100 / 600 ]  g_loss: 3.698639  d_loss: 0.709802\n",
            "[ 101 / 600 ]  g_loss: 4.453717  d_loss: 0.430531\n",
            "[ 102 / 600 ]  g_loss: 4.029628  d_loss: 0.507170\n",
            "[ 103 / 600 ]  g_loss: 2.561345  d_loss: 0.639022\n",
            "[ 104 / 600 ]  g_loss: 1.484137  d_loss: 0.478319\n",
            "[ 105 / 600 ]  g_loss: 1.463590  d_loss: 0.546522\n",
            "[ 106 / 600 ]  g_loss: 2.125216  d_loss: 0.560677\n",
            "[ 107 / 600 ]  g_loss: 3.313705  d_loss: 0.375210\n",
            "[ 108 / 600 ]  g_loss: 3.861539  d_loss: 0.646473\n",
            "[ 109 / 600 ]  g_loss: 3.202408  d_loss: 0.542228\n",
            "[ 110 / 600 ]  g_loss: 2.225217  d_loss: 0.461209\n",
            "[ 111 / 600 ]  g_loss: 1.517452  d_loss: 0.414040\n",
            "[ 112 / 600 ]  g_loss: 2.061722  d_loss: 0.573572\n",
            "[ 113 / 600 ]  g_loss: 2.835051  d_loss: 0.492649\n",
            "[ 114 / 600 ]  g_loss: 3.856767  d_loss: 0.310243\n",
            "[ 115 / 600 ]  g_loss: 4.044641  d_loss: 0.510995\n",
            "[ 116 / 600 ]  g_loss: 3.547285  d_loss: 0.520286\n",
            "[ 117 / 600 ]  g_loss: 2.761542  d_loss: 0.430089\n",
            "[ 118 / 600 ]  g_loss: 2.093295  d_loss: 0.370189\n",
            "[ 119 / 600 ]  g_loss: 2.037890  d_loss: 0.585133\n",
            "[ 120 / 600 ]  g_loss: 2.490355  d_loss: 0.572920\n",
            "[ 121 / 600 ]  g_loss: 3.596712  d_loss: 0.414263\n",
            "[ 122 / 600 ]  g_loss: 3.747255  d_loss: 0.485966\n",
            "[ 123 / 600 ]  g_loss: 3.545868  d_loss: 0.453637\n",
            "[ 124 / 600 ]  g_loss: 2.944796  d_loss: 0.566460\n",
            "[ 125 / 600 ]  g_loss: 2.129723  d_loss: 0.327068\n",
            "[ 126 / 600 ]  g_loss: 1.570908  d_loss: 0.444575\n",
            "[ 127 / 600 ]  g_loss: 2.136342  d_loss: 0.553573\n",
            "[ 128 / 600 ]  g_loss: 2.707459  d_loss: 0.511594\n",
            "[ 129 / 600 ]  g_loss: 3.133053  d_loss: 0.334138\n",
            "[ 130 / 600 ]  g_loss: 3.438403  d_loss: 0.374948\n",
            "[ 131 / 600 ]  g_loss: 2.640009  d_loss: 0.457186\n",
            "[ 132 / 600 ]  g_loss: 2.252329  d_loss: 0.417878\n",
            "[ 133 / 600 ]  g_loss: 2.000044  d_loss: 0.387159\n",
            "[ 134 / 600 ]  g_loss: 2.209007  d_loss: 0.450005\n",
            "[ 135 / 600 ]  g_loss: 2.668122  d_loss: 0.479359\n",
            "[ 136 / 600 ]  g_loss: 2.892102  d_loss: 0.342667\n",
            "[ 137 / 600 ]  g_loss: 2.593367  d_loss: 0.528617\n",
            "[ 138 / 600 ]  g_loss: 2.338978  d_loss: 0.420549\n",
            "[ 139 / 600 ]  g_loss: 2.023458  d_loss: 0.408711\n",
            "[ 140 / 600 ]  g_loss: 2.055908  d_loss: 0.438650\n",
            "[ 141 / 600 ]  g_loss: 2.557529  d_loss: 0.438591\n",
            "[ 142 / 600 ]  g_loss: 2.681224  d_loss: 0.461255\n",
            "[ 143 / 600 ]  g_loss: 2.879263  d_loss: 0.457600\n",
            "[ 144 / 600 ]  g_loss: 2.609589  d_loss: 0.429912\n",
            "[ 145 / 600 ]  g_loss: 2.491941  d_loss: 0.367220\n",
            "[ 146 / 600 ]  g_loss: 2.174286  d_loss: 0.491374\n",
            "[ 147 / 600 ]  g_loss: 2.136543  d_loss: 0.513796\n",
            "[ 148 / 600 ]  g_loss: 2.358722  d_loss: 0.583358\n",
            "[ 149 / 600 ]  g_loss: 2.711479  d_loss: 0.370301\n",
            "[ 150 / 600 ]  g_loss: 3.052198  d_loss: 0.215188\n",
            "[ 151 / 600 ]  g_loss: 2.681116  d_loss: 0.510226\n",
            "[ 152 / 600 ]  g_loss: 2.413877  d_loss: 0.472564\n",
            "[ 153 / 600 ]  g_loss: 1.860143  d_loss: 0.571274\n",
            "[ 154 / 600 ]  g_loss: 2.300358  d_loss: 0.419133\n",
            "[ 155 / 600 ]  g_loss: 2.266628  d_loss: 0.498286\n",
            "[ 156 / 600 ]  g_loss: 2.822159  d_loss: 0.403848\n",
            "[ 157 / 600 ]  g_loss: 2.837161  d_loss: 0.517239\n",
            "[ 158 / 600 ]  g_loss: 2.648633  d_loss: 0.387221\n",
            "[ 159 / 600 ]  g_loss: 2.593677  d_loss: 0.329841\n",
            "[ 160 / 600 ]  g_loss: 2.185581  d_loss: 0.397081\n",
            "[ 161 / 600 ]  g_loss: 2.192986  d_loss: 0.417035\n",
            "[ 162 / 600 ]  g_loss: 2.483126  d_loss: 0.365268\n",
            "[ 163 / 600 ]  g_loss: 2.345537  d_loss: 0.448799\n",
            "[ 164 / 600 ]  g_loss: 2.199230  d_loss: 0.393080\n",
            "[ 165 / 600 ]  g_loss: 2.360462  d_loss: 0.418156\n",
            "[ 166 / 600 ]  g_loss: 2.531882  d_loss: 0.370794\n",
            "[ 167 / 600 ]  g_loss: 2.592350  d_loss: 0.336450\n",
            "[ 168 / 600 ]  g_loss: 2.649743  d_loss: 0.380293\n",
            "[ 169 / 600 ]  g_loss: 2.776982  d_loss: 0.375596\n",
            "[ 170 / 600 ]  g_loss: 2.364430  d_loss: 0.432198\n",
            "[ 171 / 600 ]  g_loss: 2.105729  d_loss: 0.364653\n",
            "[ 172 / 600 ]  g_loss: 2.426759  d_loss: 0.211064\n",
            "[ 173 / 600 ]  g_loss: 2.809147  d_loss: 0.340135\n",
            "[ 174 / 600 ]  g_loss: 3.202945  d_loss: 0.346061\n",
            "[ 175 / 600 ]  g_loss: 3.211103  d_loss: 0.310265\n",
            "[ 176 / 600 ]  g_loss: 2.531215  d_loss: 0.494333\n",
            "[ 177 / 600 ]  g_loss: 2.374920  d_loss: 0.333023\n",
            "[ 178 / 600 ]  g_loss: 2.199574  d_loss: 0.439390\n",
            "[ 179 / 600 ]  g_loss: 2.577384  d_loss: 0.382282\n",
            "[ 180 / 600 ]  g_loss: 2.796910  d_loss: 0.357247\n",
            "[ 181 / 600 ]  g_loss: 2.746277  d_loss: 0.420930\n",
            "[ 182 / 600 ]  g_loss: 2.592821  d_loss: 0.359842\n",
            "[ 183 / 600 ]  g_loss: 2.665327  d_loss: 0.431389\n",
            "[ 184 / 600 ]  g_loss: 2.140019  d_loss: 0.548476\n",
            "[ 185 / 600 ]  g_loss: 2.325263  d_loss: 0.350976\n",
            "[ 186 / 600 ]  g_loss: 2.529386  d_loss: 0.493980\n",
            "[ 187 / 600 ]  g_loss: 2.872646  d_loss: 0.434942\n",
            "[ 188 / 600 ]  g_loss: 2.698432  d_loss: 0.380922\n",
            "[ 189 / 600 ]  g_loss: 2.747357  d_loss: 0.295207\n",
            "[ 190 / 600 ]  g_loss: 2.547128  d_loss: 0.400407\n",
            "[ 191 / 600 ]  g_loss: 2.287418  d_loss: 0.439701\n",
            "[ 192 / 600 ]  g_loss: 2.546728  d_loss: 0.327175\n",
            "[ 193 / 600 ]  g_loss: 2.665213  d_loss: 0.476319\n",
            "[ 194 / 600 ]  g_loss: 2.962280  d_loss: 0.328834\n",
            "[ 195 / 600 ]  g_loss: 2.322600  d_loss: 0.410596\n",
            "[ 196 / 600 ]  g_loss: 2.294514  d_loss: 0.410295\n",
            "[ 197 / 600 ]  g_loss: 1.884144  d_loss: 0.444600\n",
            "[ 198 / 600 ]  g_loss: 2.423134  d_loss: 0.388956\n",
            "[ 199 / 600 ]  g_loss: 3.068114  d_loss: 0.332135\n",
            "[ 200 / 600 ]  g_loss: 2.906935  d_loss: 0.545685\n",
            "[ 201 / 600 ]  g_loss: 2.568424  d_loss: 0.409820\n",
            "[ 202 / 600 ]  g_loss: 2.252106  d_loss: 0.402256\n",
            "[ 203 / 600 ]  g_loss: 2.508640  d_loss: 0.416927\n",
            "[ 204 / 600 ]  g_loss: 2.579053  d_loss: 0.369425\n",
            "[ 205 / 600 ]  g_loss: 2.630659  d_loss: 0.345150\n",
            "[ 206 / 600 ]  g_loss: 2.638346  d_loss: 0.341734\n",
            "[ 207 / 600 ]  g_loss: 2.060143  d_loss: 0.460985\n",
            "[ 208 / 600 ]  g_loss: 2.052530  d_loss: 0.376011\n",
            "[ 209 / 600 ]  g_loss: 2.154859  d_loss: 0.413047\n",
            "[ 210 / 600 ]  g_loss: 2.707488  d_loss: 0.401518\n",
            "[ 211 / 600 ]  g_loss: 2.657975  d_loss: 0.442800\n",
            "[ 212 / 600 ]  g_loss: 2.651247  d_loss: 0.413993\n",
            "[ 213 / 600 ]  g_loss: 2.411580  d_loss: 0.388790\n",
            "[ 214 / 600 ]  g_loss: 2.419521  d_loss: 0.434547\n",
            "[ 215 / 600 ]  g_loss: 2.664514  d_loss: 0.325383\n",
            "[ 216 / 600 ]  g_loss: 2.555076  d_loss: 0.509242\n",
            "[ 217 / 600 ]  g_loss: 2.462118  d_loss: 0.444145\n",
            "[ 218 / 600 ]  g_loss: 2.327761  d_loss: 0.447571\n",
            "[ 219 / 600 ]  g_loss: 2.064240  d_loss: 0.443414\n",
            "[ 220 / 600 ]  g_loss: 2.657425  d_loss: 0.456307\n",
            "[ 221 / 600 ]  g_loss: 3.109563  d_loss: 0.397377\n",
            "[ 222 / 600 ]  g_loss: 2.356288  d_loss: 0.586722\n",
            "[ 223 / 600 ]  g_loss: 2.255172  d_loss: 0.327528\n",
            "[ 224 / 600 ]  g_loss: 2.497115  d_loss: 0.329152\n",
            "[ 225 / 600 ]  g_loss: 2.882060  d_loss: 0.467407\n",
            "[ 226 / 600 ]  g_loss: 3.017915  d_loss: 0.348454\n",
            "[ 227 / 600 ]  g_loss: 2.874698  d_loss: 0.439138\n",
            "[ 228 / 600 ]  g_loss: 2.247857  d_loss: 0.478689\n",
            "[ 229 / 600 ]  g_loss: 1.858341  d_loss: 0.350873\n",
            "[ 230 / 600 ]  g_loss: 1.985321  d_loss: 0.437869\n",
            "[ 231 / 600 ]  g_loss: 2.931906  d_loss: 0.467630\n",
            "[ 232 / 600 ]  g_loss: 3.589998  d_loss: 0.428423\n",
            "[ 233 / 600 ]  g_loss: 2.891603  d_loss: 0.478927\n",
            "[ 234 / 600 ]  g_loss: 2.156768  d_loss: 0.382737\n",
            "[ 235 / 600 ]  g_loss: 1.856454  d_loss: 0.371115\n",
            "[ 236 / 600 ]  g_loss: 2.238919  d_loss: 0.378391\n",
            "[ 237 / 600 ]  g_loss: 3.361650  d_loss: 0.281516\n",
            "[ 238 / 600 ]  g_loss: 4.073513  d_loss: 0.424686\n",
            "[ 239 / 600 ]  g_loss: 3.278559  d_loss: 0.584474\n",
            "[ 240 / 600 ]  g_loss: 2.338916  d_loss: 0.304123\n",
            "[ 241 / 600 ]  g_loss: 1.808685  d_loss: 0.415925\n",
            "[ 242 / 600 ]  g_loss: 2.021955  d_loss: 0.641816\n",
            "[ 243 / 600 ]  g_loss: 2.717620  d_loss: 0.472311\n",
            "[ 244 / 600 ]  g_loss: 3.146739  d_loss: 0.440016\n",
            "[ 245 / 600 ]  g_loss: 3.129498  d_loss: 0.473921\n",
            "[ 246 / 600 ]  g_loss: 2.774731  d_loss: 0.342479\n",
            "[ 247 / 600 ]  g_loss: 2.429868  d_loss: 0.509226\n",
            "[ 248 / 600 ]  g_loss: 2.162333  d_loss: 0.372000\n",
            "[ 249 / 600 ]  g_loss: 2.081557  d_loss: 0.594738\n",
            "[ 250 / 600 ]  g_loss: 2.536352  d_loss: 0.386619\n",
            "[ 251 / 600 ]  g_loss: 2.991721  d_loss: 0.391157\n",
            "[ 252 / 600 ]  g_loss: 2.626995  d_loss: 0.442500\n",
            "[ 253 / 600 ]  g_loss: 2.337159  d_loss: 0.500779\n",
            "[ 254 / 600 ]  g_loss: 1.692547  d_loss: 0.430609\n",
            "[ 255 / 600 ]  g_loss: 2.429705  d_loss: 0.487518\n",
            "[ 256 / 600 ]  g_loss: 3.276080  d_loss: 0.410649\n",
            "[ 257 / 600 ]  g_loss: 3.480040  d_loss: 0.469576\n",
            "[ 258 / 600 ]  g_loss: 2.751591  d_loss: 0.463661\n",
            "[ 259 / 600 ]  g_loss: 1.810468  d_loss: 0.444510\n",
            "[ 260 / 600 ]  g_loss: 1.588235  d_loss: 0.474016\n",
            "[ 261 / 600 ]  g_loss: 2.696775  d_loss: 0.444150\n",
            "[ 262 / 600 ]  g_loss: 4.042063  d_loss: 0.313740\n",
            "[ 263 / 600 ]  g_loss: 4.971483  d_loss: 0.448712\n",
            "[ 264 / 600 ]  g_loss: 3.353996  d_loss: 0.843558\n",
            "[ 265 / 600 ]  g_loss: 1.573238  d_loss: 0.581817\n",
            "[ 266 / 600 ]  g_loss: 1.191968  d_loss: 0.605671\n",
            "[ 267 / 600 ]  g_loss: 2.320713  d_loss: 0.641013\n",
            "[ 268 / 600 ]  g_loss: 3.511247  d_loss: 0.455822\n",
            "[ 269 / 600 ]  g_loss: 4.378302  d_loss: 0.340713\n",
            "[ 270 / 600 ]  g_loss: 4.462202  d_loss: 0.498447\n",
            "[ 271 / 600 ]  g_loss: 3.431324  d_loss: 0.536237\n",
            "[ 272 / 600 ]  g_loss: 1.965249  d_loss: 0.504743\n",
            "[ 273 / 600 ]  g_loss: 1.523485  d_loss: 0.482633\n",
            "[ 274 / 600 ]  g_loss: 1.928269  d_loss: 0.421919\n",
            "[ 275 / 600 ]  g_loss: 3.335718  d_loss: 0.485241\n",
            "[ 276 / 600 ]  g_loss: 4.104947  d_loss: 0.477031\n",
            "[ 277 / 600 ]  g_loss: 3.354008  d_loss: 0.717638\n",
            "[ 278 / 600 ]  g_loss: 2.346142  d_loss: 0.364469\n",
            "[ 279 / 600 ]  g_loss: 1.829420  d_loss: 0.490554\n",
            "[ 280 / 600 ]  g_loss: 1.930930  d_loss: 0.546118\n",
            "[ 281 / 600 ]  g_loss: 2.829280  d_loss: 0.376291\n",
            "[ 282 / 600 ]  g_loss: 3.171567  d_loss: 0.480362\n",
            "[ 283 / 600 ]  g_loss: 3.311816  d_loss: 0.448659\n",
            "[ 284 / 600 ]  g_loss: 2.402105  d_loss: 0.493572\n",
            "[ 285 / 600 ]  g_loss: 1.565542  d_loss: 0.495967\n",
            "[ 286 / 600 ]  g_loss: 1.721429  d_loss: 0.386151\n",
            "[ 287 / 600 ]  g_loss: 2.468059  d_loss: 0.457977\n",
            "[ 288 / 600 ]  g_loss: 2.945347  d_loss: 0.482881\n",
            "[ 289 / 600 ]  g_loss: 3.611794  d_loss: 0.450684\n",
            "[ 290 / 600 ]  g_loss: 3.107409  d_loss: 0.558682\n",
            "[ 291 / 600 ]  g_loss: 2.328814  d_loss: 0.482416\n",
            "[ 292 / 600 ]  g_loss: 2.005861  d_loss: 0.388267\n",
            "[ 293 / 600 ]  g_loss: 2.106638  d_loss: 0.416393\n",
            "[ 294 / 600 ]  g_loss: 2.249071  d_loss: 0.411736\n",
            "[ 295 / 600 ]  g_loss: 2.686354  d_loss: 0.361338\n",
            "[ 296 / 600 ]  g_loss: 3.038950  d_loss: 0.386877\n",
            "[ 297 / 600 ]  g_loss: 2.676121  d_loss: 0.502944\n",
            "[ 298 / 600 ]  g_loss: 2.075656  d_loss: 0.544607\n",
            "[ 299 / 600 ]  g_loss: 1.853074  d_loss: 0.445526\n",
            "[ 300 / 600 ]  g_loss: 2.112908  d_loss: 0.530914\n",
            "[ 301 / 600 ]  g_loss: 2.872615  d_loss: 0.419459\n",
            "[ 302 / 600 ]  g_loss: 3.009308  d_loss: 0.463774\n",
            "[ 303 / 600 ]  g_loss: 2.574192  d_loss: 0.491282\n",
            "[ 304 / 600 ]  g_loss: 2.531571  d_loss: 0.382311\n",
            "[ 305 / 600 ]  g_loss: 1.921732  d_loss: 0.535579\n",
            "[ 306 / 600 ]  g_loss: 2.043917  d_loss: 0.503108\n",
            "[ 307 / 600 ]  g_loss: 2.447427  d_loss: 0.429207\n",
            "[ 308 / 600 ]  g_loss: 2.774116  d_loss: 0.382337\n",
            "[ 309 / 600 ]  g_loss: 2.215782  d_loss: 0.489231\n",
            "[ 310 / 600 ]  g_loss: 2.059008  d_loss: 0.431316\n",
            "[ 311 / 600 ]  g_loss: 2.074814  d_loss: 0.365595\n",
            "[ 312 / 600 ]  g_loss: 2.379305  d_loss: 0.457516\n",
            "[ 313 / 600 ]  g_loss: 2.476709  d_loss: 0.516729\n",
            "[ 314 / 600 ]  g_loss: 2.431393  d_loss: 0.407645\n",
            "[ 315 / 600 ]  g_loss: 2.498553  d_loss: 0.324527\n",
            "[ 316 / 600 ]  g_loss: 2.460629  d_loss: 0.485582\n",
            "[ 317 / 600 ]  g_loss: 2.442132  d_loss: 0.357102\n",
            "[ 318 / 600 ]  g_loss: 2.543318  d_loss: 0.375627\n",
            "[ 319 / 600 ]  g_loss: 2.422342  d_loss: 0.389019\n",
            "[ 320 / 600 ]  g_loss: 2.343394  d_loss: 0.488633\n",
            "[ 321 / 600 ]  g_loss: 2.308053  d_loss: 0.377438\n",
            "[ 322 / 600 ]  g_loss: 2.586649  d_loss: 0.323478\n",
            "[ 323 / 600 ]  g_loss: 3.013581  d_loss: 0.480997\n",
            "[ 324 / 600 ]  g_loss: 3.038622  d_loss: 0.339901\n",
            "[ 325 / 600 ]  g_loss: 2.369538  d_loss: 0.428862\n",
            "[ 326 / 600 ]  g_loss: 2.264825  d_loss: 0.346905\n",
            "[ 327 / 600 ]  g_loss: 2.464563  d_loss: 0.370257\n",
            "[ 328 / 600 ]  g_loss: 2.654640  d_loss: 0.354853\n",
            "[ 329 / 600 ]  g_loss: 3.248219  d_loss: 0.279578\n",
            "[ 330 / 600 ]  g_loss: 2.974755  d_loss: 0.516581\n",
            "[ 331 / 600 ]  g_loss: 2.480587  d_loss: 0.486668\n",
            "[ 332 / 600 ]  g_loss: 1.784332  d_loss: 0.428060\n",
            "[ 333 / 600 ]  g_loss: 1.962818  d_loss: 0.502568\n",
            "[ 334 / 600 ]  g_loss: 2.618785  d_loss: 0.348878\n",
            "[ 335 / 600 ]  g_loss: 3.186691  d_loss: 0.358563\n",
            "[ 336 / 600 ]  g_loss: 3.601478  d_loss: 0.294368\n",
            "[ 337 / 600 ]  g_loss: 3.079178  d_loss: 0.352677\n",
            "[ 338 / 600 ]  g_loss: 2.478055  d_loss: 0.510502\n",
            "[ 339 / 600 ]  g_loss: 1.968079  d_loss: 0.433269\n",
            "[ 340 / 600 ]  g_loss: 2.255047  d_loss: 0.397453\n",
            "[ 341 / 600 ]  g_loss: 2.800169  d_loss: 0.356883\n",
            "[ 342 / 600 ]  g_loss: 3.193680  d_loss: 0.361044\n",
            "[ 343 / 600 ]  g_loss: 3.187153  d_loss: 0.398746\n",
            "[ 344 / 600 ]  g_loss: 2.682560  d_loss: 0.413077\n",
            "[ 345 / 600 ]  g_loss: 2.430590  d_loss: 0.353731\n",
            "[ 346 / 600 ]  g_loss: 2.171928  d_loss: 0.392978\n",
            "[ 347 / 600 ]  g_loss: 2.619409  d_loss: 0.434966\n",
            "[ 348 / 600 ]  g_loss: 2.918390  d_loss: 0.337902\n",
            "[ 349 / 600 ]  g_loss: 2.661270  d_loss: 0.459398\n",
            "[ 350 / 600 ]  g_loss: 2.701166  d_loss: 0.366607\n",
            "[ 351 / 600 ]  g_loss: 2.321740  d_loss: 0.391369\n",
            "[ 352 / 600 ]  g_loss: 2.684192  d_loss: 0.395689\n",
            "[ 353 / 600 ]  g_loss: 2.549916  d_loss: 0.406948\n",
            "[ 354 / 600 ]  g_loss: 2.562807  d_loss: 0.367038\n",
            "[ 355 / 600 ]  g_loss: 2.476218  d_loss: 0.359432\n",
            "[ 356 / 600 ]  g_loss: 2.423028  d_loss: 0.422778\n",
            "[ 357 / 600 ]  g_loss: 2.425422  d_loss: 0.336325\n",
            "[ 358 / 600 ]  g_loss: 2.319090  d_loss: 0.367745\n",
            "[ 359 / 600 ]  g_loss: 2.165460  d_loss: 0.504015\n",
            "[ 360 / 600 ]  g_loss: 2.425987  d_loss: 0.499318\n",
            "[ 361 / 600 ]  g_loss: 2.754472  d_loss: 0.431309\n",
            "[ 362 / 600 ]  g_loss: 2.604328  d_loss: 0.501340\n",
            "[ 363 / 600 ]  g_loss: 2.362273  d_loss: 0.528360\n",
            "[ 364 / 600 ]  g_loss: 2.207178  d_loss: 0.349993\n",
            "[ 365 / 600 ]  g_loss: 2.279378  d_loss: 0.395424\n",
            "[ 366 / 600 ]  g_loss: 2.694136  d_loss: 0.422685\n",
            "[ 367 / 600 ]  g_loss: 3.262952  d_loss: 0.388238\n",
            "[ 368 / 600 ]  g_loss: 3.160247  d_loss: 0.558352\n",
            "[ 369 / 600 ]  g_loss: 2.279381  d_loss: 0.483324\n",
            "[ 370 / 600 ]  g_loss: 1.897326  d_loss: 0.402658\n",
            "[ 371 / 600 ]  g_loss: 2.209666  d_loss: 0.515340\n",
            "[ 372 / 600 ]  g_loss: 2.555719  d_loss: 0.437826\n",
            "[ 373 / 600 ]  g_loss: 2.813607  d_loss: 0.456008\n",
            "[ 374 / 600 ]  g_loss: 2.588472  d_loss: 0.432984\n",
            "[ 375 / 600 ]  g_loss: 2.477483  d_loss: 0.381011\n",
            "[ 376 / 600 ]  g_loss: 2.655923  d_loss: 0.364789\n",
            "[ 377 / 600 ]  g_loss: 2.572111  d_loss: 0.391113\n",
            "[ 378 / 600 ]  g_loss: 2.474726  d_loss: 0.390769\n",
            "[ 379 / 600 ]  g_loss: 2.682677  d_loss: 0.359297\n",
            "[ 380 / 600 ]  g_loss: 2.522667  d_loss: 0.368387\n",
            "[ 381 / 600 ]  g_loss: 2.851958  d_loss: 0.393671\n",
            "[ 382 / 600 ]  g_loss: 2.721538  d_loss: 0.358706\n",
            "[ 383 / 600 ]  g_loss: 2.740681  d_loss: 0.467229\n",
            "[ 384 / 600 ]  g_loss: 2.490160  d_loss: 0.417152\n",
            "[ 385 / 600 ]  g_loss: 2.345077  d_loss: 0.424643\n",
            "[ 386 / 600 ]  g_loss: 2.402246  d_loss: 0.360126\n",
            "[ 387 / 600 ]  g_loss: 2.210535  d_loss: 0.382993\n",
            "[ 388 / 600 ]  g_loss: 2.772594  d_loss: 0.335565\n",
            "[ 389 / 600 ]  g_loss: 2.822090  d_loss: 0.402081\n",
            "[ 390 / 600 ]  g_loss: 3.110060  d_loss: 0.277236\n",
            "[ 391 / 600 ]  g_loss: 3.092388  d_loss: 0.421465\n",
            "[ 392 / 600 ]  g_loss: 2.517596  d_loss: 0.482131\n",
            "[ 393 / 600 ]  g_loss: 2.308427  d_loss: 0.542082\n",
            "[ 394 / 600 ]  g_loss: 2.339714  d_loss: 0.414747\n",
            "[ 395 / 600 ]  g_loss: 2.898859  d_loss: 0.406002\n",
            "[ 396 / 600 ]  g_loss: 3.243298  d_loss: 0.464588\n",
            "[ 397 / 600 ]  g_loss: 3.387348  d_loss: 0.272581\n",
            "[ 398 / 600 ]  g_loss: 2.876268  d_loss: 0.288544\n",
            "[ 399 / 600 ]  g_loss: 2.337241  d_loss: 0.384107\n",
            "[ 400 / 600 ]  g_loss: 1.867251  d_loss: 0.481824\n",
            "[ 401 / 600 ]  g_loss: 1.993632  d_loss: 0.412775\n",
            "[ 402 / 600 ]  g_loss: 2.885143  d_loss: 0.485541\n",
            "[ 403 / 600 ]  g_loss: 2.996688  d_loss: 0.489276\n",
            "[ 404 / 600 ]  g_loss: 2.913995  d_loss: 0.489237\n",
            "[ 405 / 600 ]  g_loss: 2.100345  d_loss: 0.433629\n",
            "[ 406 / 600 ]  g_loss: 2.032233  d_loss: 0.502194\n",
            "[ 407 / 600 ]  g_loss: 2.693134  d_loss: 0.371551\n",
            "[ 408 / 600 ]  g_loss: 2.946944  d_loss: 0.469751\n",
            "[ 409 / 600 ]  g_loss: 3.046169  d_loss: 0.364748\n",
            "[ 410 / 600 ]  g_loss: 2.591199  d_loss: 0.298558\n",
            "[ 411 / 600 ]  g_loss: 2.324329  d_loss: 0.340184\n",
            "[ 412 / 600 ]  g_loss: 1.900615  d_loss: 0.471116\n",
            "[ 413 / 600 ]  g_loss: 1.829707  d_loss: 0.476254\n",
            "[ 414 / 600 ]  g_loss: 2.729350  d_loss: 0.372107\n",
            "[ 415 / 600 ]  g_loss: 2.902646  d_loss: 0.466564\n",
            "[ 416 / 600 ]  g_loss: 2.913433  d_loss: 0.480554\n",
            "[ 417 / 600 ]  g_loss: 2.366620  d_loss: 0.368793\n",
            "[ 418 / 600 ]  g_loss: 1.971432  d_loss: 0.430866\n",
            "[ 419 / 600 ]  g_loss: 2.514338  d_loss: 0.435515\n",
            "[ 420 / 600 ]  g_loss: 3.310338  d_loss: 0.329905\n",
            "[ 421 / 600 ]  g_loss: 3.076945  d_loss: 0.467641\n",
            "[ 422 / 600 ]  g_loss: 2.514363  d_loss: 0.489624\n",
            "[ 423 / 600 ]  g_loss: 2.250482  d_loss: 0.384977\n",
            "[ 424 / 600 ]  g_loss: 2.460267  d_loss: 0.464935\n",
            "[ 425 / 600 ]  g_loss: 2.964198  d_loss: 0.385177\n",
            "[ 426 / 600 ]  g_loss: 3.142950  d_loss: 0.376686\n",
            "[ 427 / 600 ]  g_loss: 2.751752  d_loss: 0.498450\n",
            "[ 428 / 600 ]  g_loss: 2.485628  d_loss: 0.370942\n",
            "[ 429 / 600 ]  g_loss: 2.633656  d_loss: 0.381774\n",
            "[ 430 / 600 ]  g_loss: 2.680387  d_loss: 0.466249\n",
            "[ 431 / 600 ]  g_loss: 2.169558  d_loss: 0.487053\n",
            "[ 432 / 600 ]  g_loss: 2.394201  d_loss: 0.307960\n",
            "[ 433 / 600 ]  g_loss: 2.288714  d_loss: 0.465791\n",
            "[ 434 / 600 ]  g_loss: 2.521730  d_loss: 0.401435\n",
            "[ 435 / 600 ]  g_loss: 2.711647  d_loss: 0.386676\n",
            "[ 436 / 600 ]  g_loss: 2.476821  d_loss: 0.380176\n",
            "[ 437 / 600 ]  g_loss: 2.422456  d_loss: 0.352140\n",
            "[ 438 / 600 ]  g_loss: 2.450298  d_loss: 0.439569\n",
            "[ 439 / 600 ]  g_loss: 2.550372  d_loss: 0.352221\n",
            "[ 440 / 600 ]  g_loss: 2.636047  d_loss: 0.391280\n",
            "[ 441 / 600 ]  g_loss: 2.322728  d_loss: 0.353467\n",
            "[ 442 / 600 ]  g_loss: 2.155026  d_loss: 0.453858\n",
            "[ 443 / 600 ]  g_loss: 2.228943  d_loss: 0.399692\n",
            "[ 444 / 600 ]  g_loss: 2.474130  d_loss: 0.372045\n",
            "[ 445 / 600 ]  g_loss: 3.080276  d_loss: 0.333233\n",
            "[ 446 / 600 ]  g_loss: 2.938202  d_loss: 0.439305\n",
            "[ 447 / 600 ]  g_loss: 2.660046  d_loss: 0.349012\n",
            "[ 448 / 600 ]  g_loss: 2.104341  d_loss: 0.428448\n",
            "[ 449 / 600 ]  g_loss: 2.091100  d_loss: 0.463956\n",
            "[ 450 / 600 ]  g_loss: 2.621385  d_loss: 0.477664\n",
            "[ 451 / 600 ]  g_loss: 3.345965  d_loss: 0.370541\n",
            "[ 452 / 600 ]  g_loss: 3.086468  d_loss: 0.433666\n",
            "[ 453 / 600 ]  g_loss: 2.428729  d_loss: 0.455147\n",
            "[ 454 / 600 ]  g_loss: 2.077468  d_loss: 0.459647\n",
            "[ 455 / 600 ]  g_loss: 2.464809  d_loss: 0.558440\n",
            "[ 456 / 600 ]  g_loss: 2.835566  d_loss: 0.429445\n",
            "[ 457 / 600 ]  g_loss: 2.896521  d_loss: 0.461320\n",
            "[ 458 / 600 ]  g_loss: 2.778604  d_loss: 0.439500\n",
            "[ 459 / 600 ]  g_loss: 2.402008  d_loss: 0.413544\n",
            "[ 460 / 600 ]  g_loss: 2.577850  d_loss: 0.342604\n",
            "[ 461 / 600 ]  g_loss: 2.092978  d_loss: 0.509595\n",
            "[ 462 / 600 ]  g_loss: 2.009350  d_loss: 0.441737\n",
            "[ 463 / 600 ]  g_loss: 2.594247  d_loss: 0.424150\n",
            "[ 464 / 600 ]  g_loss: 2.784725  d_loss: 0.427322\n",
            "[ 465 / 600 ]  g_loss: 2.779357  d_loss: 0.424718\n",
            "[ 466 / 600 ]  g_loss: 2.401926  d_loss: 0.385802\n",
            "[ 467 / 600 ]  g_loss: 2.486432  d_loss: 0.356703\n",
            "[ 468 / 600 ]  g_loss: 2.379992  d_loss: 0.453801\n",
            "[ 469 / 600 ]  g_loss: 2.537838  d_loss: 0.373044\n",
            "[ 470 / 600 ]  g_loss: 2.682753  d_loss: 0.457129\n",
            "[ 471 / 600 ]  g_loss: 2.722830  d_loss: 0.400966\n",
            "[ 472 / 600 ]  g_loss: 2.385656  d_loss: 0.288231\n",
            "[ 473 / 600 ]  g_loss: 2.603040  d_loss: 0.342302\n",
            "[ 474 / 600 ]  g_loss: 2.692270  d_loss: 0.373093\n",
            "[ 475 / 600 ]  g_loss: 2.927486  d_loss: 0.387762\n",
            "[ 476 / 600 ]  g_loss: 2.670110  d_loss: 0.427387\n",
            "[ 477 / 600 ]  g_loss: 2.199433  d_loss: 0.622928\n",
            "[ 478 / 600 ]  g_loss: 2.783639  d_loss: 0.397013\n",
            "[ 479 / 600 ]  g_loss: 3.200630  d_loss: 0.454345\n",
            "[ 480 / 600 ]  g_loss: 2.676207  d_loss: 0.521841\n",
            "[ 481 / 600 ]  g_loss: 2.308155  d_loss: 0.504125\n",
            "[ 482 / 600 ]  g_loss: 1.916882  d_loss: 0.377261\n",
            "[ 483 / 600 ]  g_loss: 2.102233  d_loss: 0.423936\n",
            "[ 484 / 600 ]  g_loss: 2.626942  d_loss: 0.450552\n",
            "[ 485 / 600 ]  g_loss: 2.678377  d_loss: 0.398814\n",
            "[ 486 / 600 ]  g_loss: 2.735327  d_loss: 0.497897\n",
            "[ 487 / 600 ]  g_loss: 2.701782  d_loss: 0.335084\n",
            "[ 488 / 600 ]  g_loss: 2.757003  d_loss: 0.391606\n",
            "[ 489 / 600 ]  g_loss: 2.714179  d_loss: 0.354236\n",
            "[ 490 / 600 ]  g_loss: 2.528197  d_loss: 0.459382\n",
            "[ 491 / 600 ]  g_loss: 2.346501  d_loss: 0.495232\n",
            "[ 492 / 600 ]  g_loss: 2.312397  d_loss: 0.443760\n",
            "[ 493 / 600 ]  g_loss: 2.630295  d_loss: 0.505313\n",
            "[ 494 / 600 ]  g_loss: 3.090757  d_loss: 0.290129\n",
            "[ 495 / 600 ]  g_loss: 3.422993  d_loss: 0.371490\n",
            "[ 496 / 600 ]  g_loss: 2.925126  d_loss: 0.485904\n",
            "[ 497 / 600 ]  g_loss: 2.214067  d_loss: 0.440061\n",
            "[ 498 / 600 ]  g_loss: 2.168123  d_loss: 0.344134\n",
            "[ 499 / 600 ]  g_loss: 2.667360  d_loss: 0.374390\n",
            "[ 500 / 600 ]  g_loss: 3.186779  d_loss: 0.281615\n",
            "[ 501 / 600 ]  g_loss: 3.266273  d_loss: 0.480652\n",
            "[ 502 / 600 ]  g_loss: 2.830388  d_loss: 0.459696\n",
            "[ 503 / 600 ]  g_loss: 2.180111  d_loss: 0.406230\n",
            "[ 504 / 600 ]  g_loss: 2.005464  d_loss: 0.434612\n",
            "[ 505 / 600 ]  g_loss: 2.661129  d_loss: 0.433597\n",
            "[ 506 / 600 ]  g_loss: 3.527299  d_loss: 0.360779\n",
            "[ 507 / 600 ]  g_loss: 3.332791  d_loss: 0.531508\n",
            "[ 508 / 600 ]  g_loss: 2.693377  d_loss: 0.405766\n",
            "[ 509 / 600 ]  g_loss: 1.963119  d_loss: 0.422458\n",
            "[ 510 / 600 ]  g_loss: 1.976147  d_loss: 0.424020\n",
            "[ 511 / 600 ]  g_loss: 3.043325  d_loss: 0.370695\n",
            "[ 512 / 600 ]  g_loss: 3.912993  d_loss: 0.419126\n",
            "[ 513 / 600 ]  g_loss: 3.559453  d_loss: 0.413041\n",
            "[ 514 / 600 ]  g_loss: 2.962717  d_loss: 0.467722\n",
            "[ 515 / 600 ]  g_loss: 2.318631  d_loss: 0.339175\n",
            "[ 516 / 600 ]  g_loss: 2.091532  d_loss: 0.530108\n",
            "[ 517 / 600 ]  g_loss: 2.649813  d_loss: 0.485174\n",
            "[ 518 / 600 ]  g_loss: 3.383515  d_loss: 0.324278\n",
            "[ 519 / 600 ]  g_loss: 3.503269  d_loss: 0.377568\n",
            "[ 520 / 600 ]  g_loss: 2.704026  d_loss: 0.455373\n",
            "[ 521 / 600 ]  g_loss: 1.775737  d_loss: 0.423847\n",
            "[ 522 / 600 ]  g_loss: 1.895141  d_loss: 0.464071\n",
            "[ 523 / 600 ]  g_loss: 2.765759  d_loss: 0.447727\n",
            "[ 524 / 600 ]  g_loss: 3.295614  d_loss: 0.366156\n",
            "[ 525 / 600 ]  g_loss: 3.148674  d_loss: 0.481882\n",
            "[ 526 / 600 ]  g_loss: 2.526397  d_loss: 0.439366\n",
            "[ 527 / 600 ]  g_loss: 2.063440  d_loss: 0.372091\n",
            "[ 528 / 600 ]  g_loss: 2.030856  d_loss: 0.365303\n",
            "[ 529 / 600 ]  g_loss: 2.732915  d_loss: 0.436456\n",
            "[ 530 / 600 ]  g_loss: 2.998200  d_loss: 0.327769\n",
            "[ 531 / 600 ]  g_loss: 2.784642  d_loss: 0.424842\n",
            "[ 532 / 600 ]  g_loss: 2.564487  d_loss: 0.286411\n",
            "[ 533 / 600 ]  g_loss: 2.238708  d_loss: 0.441395\n",
            "[ 534 / 600 ]  g_loss: 2.242810  d_loss: 0.359465\n",
            "[ 535 / 600 ]  g_loss: 2.541479  d_loss: 0.384016\n",
            "[ 536 / 600 ]  g_loss: 2.854590  d_loss: 0.334508\n",
            "[ 537 / 600 ]  g_loss: 2.951901  d_loss: 0.468442\n",
            "[ 538 / 600 ]  g_loss: 2.948720  d_loss: 0.329547\n",
            "[ 539 / 600 ]  g_loss: 2.529653  d_loss: 0.432835\n",
            "[ 540 / 600 ]  g_loss: 2.183788  d_loss: 0.399081\n",
            "[ 541 / 600 ]  g_loss: 2.351075  d_loss: 0.388589\n",
            "[ 542 / 600 ]  g_loss: 3.019148  d_loss: 0.368700\n",
            "[ 543 / 600 ]  g_loss: 3.647616  d_loss: 0.361752\n",
            "[ 544 / 600 ]  g_loss: 2.992149  d_loss: 0.647181\n",
            "[ 545 / 600 ]  g_loss: 2.054993  d_loss: 0.464414\n",
            "[ 546 / 600 ]  g_loss: 1.857282  d_loss: 0.381347\n",
            "[ 547 / 600 ]  g_loss: 2.716102  d_loss: 0.430525\n",
            "[ 548 / 600 ]  g_loss: 3.458333  d_loss: 0.297784\n",
            "[ 549 / 600 ]  g_loss: 3.864771  d_loss: 0.349790\n",
            "[ 550 / 600 ]  g_loss: 2.828172  d_loss: 0.642184\n",
            "[ 551 / 600 ]  g_loss: 1.663294  d_loss: 0.448573\n",
            "[ 552 / 600 ]  g_loss: 1.781232  d_loss: 0.420798\n",
            "[ 553 / 600 ]  g_loss: 2.576004  d_loss: 0.477257\n",
            "[ 554 / 600 ]  g_loss: 3.148087  d_loss: 0.416452\n",
            "[ 555 / 600 ]  g_loss: 2.688975  d_loss: 0.561797\n",
            "[ 556 / 600 ]  g_loss: 2.435552  d_loss: 0.364640\n",
            "[ 557 / 600 ]  g_loss: 2.245808  d_loss: 0.318368\n",
            "[ 558 / 600 ]  g_loss: 2.429214  d_loss: 0.378825\n",
            "[ 559 / 600 ]  g_loss: 2.496310  d_loss: 0.409635\n",
            "[ 560 / 600 ]  g_loss: 2.671017  d_loss: 0.417032\n",
            "[ 561 / 600 ]  g_loss: 2.725309  d_loss: 0.323636\n",
            "[ 562 / 600 ]  g_loss: 2.480787  d_loss: 0.435337\n",
            "[ 563 / 600 ]  g_loss: 2.514581  d_loss: 0.305349\n",
            "[ 564 / 600 ]  g_loss: 2.645984  d_loss: 0.361351\n",
            "[ 565 / 600 ]  g_loss: 2.309843  d_loss: 0.531977\n",
            "[ 566 / 600 ]  g_loss: 2.190855  d_loss: 0.497486\n",
            "[ 567 / 600 ]  g_loss: 2.491641  d_loss: 0.382626\n",
            "[ 568 / 600 ]  g_loss: 2.694294  d_loss: 0.474662\n",
            "[ 569 / 600 ]  g_loss: 2.819562  d_loss: 0.305604\n",
            "[ 570 / 600 ]  g_loss: 2.667747  d_loss: 0.474605\n",
            "[ 571 / 600 ]  g_loss: 2.830783  d_loss: 0.461751\n",
            "[ 572 / 600 ]  g_loss: 2.400347  d_loss: 0.323291\n",
            "[ 573 / 600 ]  g_loss: 2.558832  d_loss: 0.345912\n",
            "[ 574 / 600 ]  g_loss: 2.986704  d_loss: 0.380628\n",
            "[ 575 / 600 ]  g_loss: 3.173716  d_loss: 0.402121\n",
            "[ 576 / 600 ]  g_loss: 2.486081  d_loss: 0.514495\n",
            "[ 577 / 600 ]  g_loss: 2.328001  d_loss: 0.395515\n",
            "[ 578 / 600 ]  g_loss: 1.675145  d_loss: 0.452546\n",
            "[ 579 / 600 ]  g_loss: 2.485450  d_loss: 0.520098\n",
            "[ 580 / 600 ]  g_loss: 3.315099  d_loss: 0.446273\n",
            "[ 581 / 600 ]  g_loss: 3.075976  d_loss: 0.555275\n",
            "[ 582 / 600 ]  g_loss: 2.116904  d_loss: 0.523884\n",
            "[ 583 / 600 ]  g_loss: 1.200656  d_loss: 0.422027\n",
            "[ 584 / 600 ]  g_loss: 2.511501  d_loss: 0.757251\n",
            "[ 585 / 600 ]  g_loss: 3.963866  d_loss: 0.392241\n",
            "[ 586 / 600 ]  g_loss: 4.099633  d_loss: 0.534839\n",
            "[ 587 / 600 ]  g_loss: 2.751810  d_loss: 0.719028\n",
            "[ 588 / 600 ]  g_loss: 1.921978  d_loss: 0.271903\n",
            "[ 589 / 600 ]  g_loss: 1.816636  d_loss: 0.459866\n",
            "[ 590 / 600 ]  g_loss: 2.737227  d_loss: 0.372820\n",
            "[ 591 / 600 ]  g_loss: 3.750724  d_loss: 0.409102\n",
            "[ 592 / 600 ]  g_loss: 3.650502  d_loss: 0.574819\n",
            "[ 593 / 600 ]  g_loss: 3.082938  d_loss: 0.341976\n",
            "[ 594 / 600 ]  g_loss: 2.019042  d_loss: 0.527237\n",
            "[ 595 / 600 ]  g_loss: 1.507303  d_loss: 0.436391\n",
            "[ 596 / 600 ]  g_loss: 2.017826  d_loss: 0.482249\n",
            "[ 597 / 600 ]  g_loss: 2.973076  d_loss: 0.365588\n",
            "[ 598 / 600 ]  g_loss: 3.901128  d_loss: 0.291409\n",
            "[ 599 / 600 ]  g_loss: 3.747974  d_loss: 0.365003\n",
            "<class 'float'>\n",
            "<class 'float'>\n"
          ]
        }
      ]
    }
  ]
}